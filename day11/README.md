# Day 11

## Part 1
This part was pretty straightforward...just translate the instructions into code.

## Part 2
This part was more interesting, and I required a few iterations to arrive at a practical solution. The main issue here is the exponential growth in the number of stones. With 25 blinks, we could just calculate the full sequence without much thought. But with 75 blinks, the exponential growth requires a more thoughtful approach.

My first attempt was admittedly pretty lazy: if we already had a working algorithm from the previous part and it is now too slow, let's minimally modify the code to take advantage of parallel computing. We have an embarrassingly parallel problem here: each stone evolves into a new sequence independent of the other stones. So we could task each core with one of the numbers in the puzzle input and sum across the results. We can actually do a bit better than this: my machine has 20 logical cores, and my test input had 8 numbers. So we could evolve the sequence sequentially a few times until we have one initial stone for each logical core, and then utilize `multiprocessing.Pool()` to evolve each stone in parallel. The obvious problem here is that an order of magnitude increase in computation is no match for reigning in exponential growth. In my attention span of a few seconds, around 35 iterations could be completed before everything slowed to a crawl.

To improve upon this, I had to reduce the amount of redundant work. For my second attempt, I refactored the algorithm so that each stone was evolved into a new list without any in-memory modifications. To reduce redundant computation, I wrapped this function in an LRU cache (`functools.lru_cache`) so that previous results could be reused rather than requiring new computation each time. Since the LRU cache is threadsafe, I also utilized the multiprocessing approach first explored. This approach did push our ceiling to around 50 iterations before the cache was getting hammered too hard to allow for efficient additional progress. But with an exponential growth, 75 is a long way from 50, so more work was needed.

We were on the right path here with reducing redundant computation, but I realized that the previous attempt was still extremely wasteful. Although the problem statement heavily stressed details about the ordering of the stones, this was actually not relevant for our specific goal of counting the sequence length. This offers a huge shortcut: if our sequence had many instances of a given stone value, we could instead evolve this value once and just keep track of how many stones with this value we have. To illustrate why this is powerful, imagine a simple initial stone sequence of `[0]`. After 35 iterations, there are 1.3 million stones but only 54 unique stone values. All of sudden, we have relatively few calculations required and no longer require any multiprocessing or caching. This was a much more scalable solution that could handle the full 75 iterations in less than a second.
